from datetime import datetime
import hashlib
import time
import os

import requests
from bs4 import BeautifulSoup

from board import Board
from database import SupabaseManager
from gpt_client import GPTClient

CRAWLING_URL_LIST = [
"https://computer.cnu.ac.kr/computer/notice/bachelor.do",
"https://computer.cnu.ac.kr/computer/notice/notice.do",
"https://computer.cnu.ac.kr/computer/notice/project.do",
"https://computer.cnu.ac.kr/computer/notice/cse.do",
]

def b_title_box_parser (item) :
    notice_data = {}

    link = item.select_one("a")
    if link:
        notice_data['title'] = link.get_text().strip()
        notice_data['url'] = link.get('href', '')

    # ìƒˆ ê¸€ ì—¬ë¶€ í™•ì¸
    new_mark = item.select_one(".b-new span")
    notice_data['is_new'] = new_mark is not None

    # ì„¸ë¶€ ì •ë³´ ì¶”ì¶œ
    m_con = item.select_one(".b-m-con")
    if m_con:
        # ê³µì§€ ì—¬ë¶€
        notice_mark = m_con.select_one(".b-notice")
        notice_data['is_notice'] = notice_mark is not None

        # ì‘ì„±ì
        writer = m_con.select_one(".b-writer")
        notice_data['writer'] = writer.get_text().strip() if writer else ""

        # ë‚ ì§œ
        date = m_con.select_one(".b-date")
        notice_data['date'] = date.get_text().strip() if date else ""

        # ì¡°íšŒìˆ˜
        hit = m_con.select_one(".hit")
        notice_data['views'] = hit.get_text().strip().replace('ì¡°íšŒìˆ˜ ', '') if hit else "0"

    return notice_data

def make_pagination_url(page, url):
    offset = (page - 1) * 10
    return f"{url}?mode=list&&articleLimit=10&article.offset={offset}"


gpt = GPTClient()
db_manager = SupabaseManager()

def update_notice_schedules(deep_url: str, base_url: str):
    """
    ê¸°ì¡´ ê³µì§€ì‚¬í•­ì˜ ì¼ì • ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    title = ""  # for exception logging
    try:
        # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        deepResponse = requests.get(deep_url, timeout=10)
        deepResponse.raise_for_status()
        context = Board(boardHtml=deepResponse.text, baseUrl=base_url)

        title = context.title
        content = context.detail_text

        print(f"ğŸ”„ '{title}' ê³µì§€ì‚¬í•­ì˜ ì¼ì • ì •ë³´ ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", flush=True)

        # GPTë¥¼ í†µí•´ ì¼ì • ì •ë³´ ì¶”ì¶œ
        schedules = gpt.extract_schedule_from_notice(title=title, content=content)

        # ì¶”ì¶œëœ ì¼ì •ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ DB ì—…ë°ì´íŠ¸
        if schedules:
            db_manager.save_schedules(notice_title=title, schedules=schedules)
            print(f"âœ… '{title}' ê³µì§€ì‚¬í•­ì˜ ì¼ì • {len(schedules)}ê±´ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.", flush=True)
        else:
            print(f"â„¹ï¸ '{title}' ê³µì§€ì‚¬í•­ì—ì„œ ì¶”ì¶œí•  ì¼ì •ì´ ì—†ìŠµë‹ˆë‹¤.", flush=True)

    except requests.exceptions.RequestException as e:
        print(f"ğŸ”´ ìƒì„¸ í˜ì´ì§€({deep_url})ì— ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
    except Exception as e:
        error_title = f"'{title}' " if title else ""
        print(f"ğŸ”´ {error_title}ê³µì§€ì‚¬í•­ì˜ ì¼ì • ì—…ë°ì´íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ({deep_url}): {e}", flush=True)



from typing import List, Dict

def discord_web_hook(notices: List[dict]):
    """ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ë“¤ì„ Discord webhookìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    if not notices:
        return
    
    # í™œì„±í™”ëœ webhook ì´ ê°œìˆ˜ í™•ì¸
    total_webhooks = db_manager.get_active_webhooks_count()
    
    if total_webhooks == 0:
        print("â„¹ï¸ í™œì„±í™”ëœ Discord webhookì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“¢ {len(notices)}ê°œì˜ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì„ {total_webhooks}ê°œì˜ webhookìœ¼ë¡œ ë°°ì¹˜ ì „ì†¡í•©ë‹ˆë‹¤.")
    
    batch_size = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  webhook ê°œìˆ˜
    
    for notice in notices:
        markdown_content = notice.get('markdown_content') or "ìš”ì•½ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        original_url_text = f"\n\nğŸ”— **ì›ë³¸ ë§í¬**: {notice.get('original_url', '')}"
        
        payload = {
            "content": markdown_content + original_url_text
        }
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ webhook ì²˜ë¦¬
        offset = 0
        batch_count = 0
        
        while True:
            webhook_batch = db_manager.get_active_webhooks_batch(batch_size=batch_size, offset=offset)
            
            if not webhook_batch:
                break
            
            batch_count += 1
            print(f"ğŸ”„ ë°°ì¹˜ {batch_count}: {len(webhook_batch)}ê°œì˜ webhook ì²˜ë¦¬ ì¤‘...")
            
            # í˜„ì¬ ë°°ì¹˜ì˜ webhookë“¤ì— ì „ì†¡
            for webhook in webhook_batch:
                try:
                    response = requests.post(webhook.url, json=payload, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Webhook '{webhook.url[:50]}...'ì— '{notice.get('title', '')[:30]}...' ì „ì†¡ ì„±ê³µ")
                except requests.exceptions.HTTPError as e:
                    if response.status_code == 404:
                        print(f"ğŸ”´ Webhook '{webhook.url[:50]}...'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
                        db_manager.deactivate_webhook(webhook.id)
                    else:
                        print(f"âŒ Webhook '{webhook.url[:50]}...'ì— ì „ì†¡ ì‹¤íŒ¨ (HTTP {response.status_code}): {e}")
                except Exception as e:
                    print(f"âŒ Webhook ì „ì†¡ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            
            offset += batch_size
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if webhook_batch:
                time.sleep(0.1)
        
        print(f"âœ… '{notice.get('title', '')[:30]}...' ê³µì§€ì‚¬í•­ì„ ëª¨ë“  webhookì— ì „ì†¡ ì™„ë£Œ")

def triggered_notice_exists(notices: List[dict]):
    discord_web_hook(notices)

# url Stringì„ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì•„, í•´ë‹¹ ì‚¬ì´íŠ¸ htmlì„ ê¸ì–´ì™€, ì „ì²´ì ì¸ íŒŒì‹±ì„ ì‹œì‘í•˜ëŠ” í•¨ìˆ˜
def crawler(url : str, page : int, category : int):
    response = requests.get(make_pagination_url(page, url), timeout=10)
    soup = BeautifulSoup(response.text, "html.parser")

    items = soup.select("div.b-title-box")

    # title, url, is_new, is_notice, writer, date, views ë”•ì…”ë„ˆë¦¬ë¡œ ë°ì´í„° ì¡´ì¬, ë¦¬ìŠ¤íŠ¸ì„.
    items = list(map(b_title_box_parser, items))

    # print(*items, sep="\n")

    notices = []

    for item in items:
        if db_manager.notice_exists(title=item['title']) :
            # print(f"âš ï¸ '{item['title']}' ì´ì „ì— ìˆëŠ” ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤. ì¼ì • ì •ë³´ ì—…ë°ì´íŠ¸ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.", flush=True)
            
            # deepUrl = url + item['url']
            # update_notice_schedules(deep_url=deepUrl, base_url=url)

            # time.sleep(10) # GPT API í˜¸ì¶œ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëŒ€ê¸°

            print(f"âš ï¸ '{item['title']}' ì´ì „ì— ìˆëŠ” ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤.", flush=True)
            continue

        deepUrl = url + item['url']
        deepResponse : requests.api = requests.get(deepUrl, timeout=10)
        context = Board(boardHtml=deepResponse.text, baseUrl=url)

        ai_response = gpt.process_notice_content(title=context.title, content=context.detail_text)
        ai_schedules = gpt.extract_schedule_from_notice(title=context.title, content=context.detail_text)


        # 2. DBì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
        try:
            # ë‚ ì§œ ë¬¸ìì—´('YYYY-MM-DD' ë˜ëŠ” 'YYYY.MM.DD')ì„ date ê°ì²´ë¡œ ë³€í™˜
            date_str = context.date.replace('.', '-')
            publish_date_obj = datetime.strptime(date_str, '%Y-%m-%d').date()
        except (ValueError, TypeError):
            publish_date_obj = None  # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ë‚ ì§œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° Noneìœ¼ë¡œ ì²˜ë¦¬

        # models.Noticeì— ë§ëŠ” ë°ì´í„° êµ¬ì¡° ìƒì„±
        notice_data = {
            'title': context.title,
            'content': context.detail_text,
            'writer': context.writer,
            'writer_email': context.email,
            'publish_date': publish_date_obj,
            'is_notice': item.get('is_notice', False),
            'ai_summary_title': ai_response.get('ai_summary_title'),
            'ai_summary_content': ai_response.get('ai_summary_content'),
            'markdown_content': ai_response.get('markdown_content'),
            'original_url': deepUrl,
            'category': category,
        }

        # 3. ë°ì´í„°ë² ì´ìŠ¤ì— ê³µì§€ì‚¬í•­ ë° ì´ë¯¸ì§€ ì •ë³´ ì €ì¥
        try:
            notice = db_manager.save_notice(notice_data=notice_data, image_urls=context.images, files=context.file_box, ai_schedules=ai_schedules)
        except Exception as e:
            print(f"ğŸ”´ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
            
        notices.append(notice)  # noticeëŠ” dict
        time.sleep(5)
    
    triggered_notice_exists(notices)

def discord_web_hook_admin(error_message: str):
    """ê´€ë¦¬ììš© Discord Webhookìœ¼ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ ì „ì†¡"""

    admin_webhook_url = os.getenv("DISCORD_ADMIN_WEBHOOK_URL")

    if not admin_webhook_url:
        print("â— ê´€ë¦¬ììš© Discord Webhook URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    payload = {
        "content": f"ğŸš¨ **í¬ë¡¤ëŸ¬ ì—ëŸ¬ ë°œìƒ!**\n```\n{error_message}\n```"
    }

    try:
        response = requests.post(admin_webhook_url, json=payload, timeout=10)
        response.raise_for_status()
        print("âœ… ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ê´€ë¦¬ì Discord Webhookìœ¼ë¡œ ì „ì†¡í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ê´€ë¦¬ì Discord Webhook ì „ì†¡ ì‹¤íŒ¨: {e}")

def main():
    category = 0
    for url in CRAWLING_URL_LIST:
        try:
            crawler(url, 1, category)
        except Exception as e:
            error_message = f"[{url}] {str(e)}"
            discord_web_hook_admin(error_message)
            print(e, "ì—ëŸ¬ë¡œ ì¸í•´, ì‹œìŠ¤í…œ ì¤‘ì§€.")
            return
        category += 1
        time.sleep(5)

if __name__ == '__main__':
    main()